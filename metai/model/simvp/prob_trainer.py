# run/train_scwds_prob.py (æ¦‚ç‡åˆ†ç®± SimVP-Mamba è¿ç§»å­¦ä¹ è®­ç»ƒè„šæœ¬ - æ”¹è¿›ç‰ˆ)

import sys
import os
import glob
from datetime import datetime
import argparse
import ast
from pydantic import ValidationError

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import lightning as l
from lightning.pytorch.loggers import TensorBoardLogger
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor

from metai.dataset.met_dataloader_scwds import ScwdsDataModule
from metai.model.simvp.simvp_config import SimVPConfig
# å¯¼å…¥ Probabilistic Trainer å’Œ SimVP åŸºåº§æ¨¡å‹
from metai.model.simvp.prob_trainer import ProbabilisticSimVP 
from metai.model.simvp.simvp_trainer import SimVP 
from metai.utils import MLOGI

def find_best_ckpt(save_dir: str) -> str:
    """æŸ¥æ‰¾æœ€ä¼˜æˆ–æœ€æ–°çš„ Checkpoint æ–‡ä»¶ï¼Œä¼˜å…ˆ best.ckpt"""
    best = os.path.join(save_dir, 'best.ckpt')
    if os.path.exists(best): return best
    
    last = os.path.join(save_dir, 'last.ckpt')
    if os.path.exists(last): return last
    
    cpts = sorted(glob.glob(os.path.join(save_dir, '*.ckpt')))
    if len(cpts) > 0: return cpts[-1]
        
    raise FileNotFoundError(f'No checkpoint found in {save_dir}')

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Train SCWDS Probabilistic SimVP Model (Transfer Learning)')
    
    # --- åŸºç¡€è·¯å¾„ä¸æ•°æ®å‚æ•° ---
    parser.add_argument('--data_path', type=str, default='data/samples.jsonl', help='Path to training data')
    parser.add_argument('--save_dir', type=str, default='./output/prob_simvp', help='Output directory')
    parser.add_argument('--in_shape', type=int, nargs=4, default=None) 
    parser.add_argument('--batch_size', type=int, default=None)
    parser.add_argument('--max_epochs', type=int, default=30, help='å¾®è°ƒçš„æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--num_workers', type=int, default=None)
    parser.add_argument('--aft_seq_length', type=int, default=None)

    # --- [æ ¸å¿ƒæ”¹è¿›] æ¦‚ç‡åˆ†ç®±ä¸ Loss å‚æ•° ---
    parser.add_argument('--num_bins', type=int, default=40, help='æ¦‚ç‡åˆ†ç®±çš„æ•°é‡ (å»ºè®®é™ä½è‡³ 40)')
    parser.add_argument('--sigma', type=float, default=2.0, help='é«˜æ–¯è½¯æ ‡ç­¾çš„æ ‡å‡†å·® (Gaussian Soft Label Sigma)')
    parser.add_argument('--use_focal', type=str, default='true', help='æ˜¯å¦å¯ç”¨ Focal Loss (true/false)')
    parser.add_argument('--gamma', type=float, default=2.0, help='Focal Loss çš„èšç„¦å‚æ•°')

    # --- [è¿ç§»å­¦ä¹ å‚æ•°] ---
    parser.add_argument('--base_ckpt_dir', type=str, required=True, help='SimVPåŸºåº§æ¨¡å‹ç›®å½•ï¼Œç”¨äºåŠ è½½ backbone')
    parser.add_argument('--ckpt_path', type=str, default=None, help='ç›´æ¥åŠ è½½ç‰¹å®šæƒé‡çš„è·¯å¾„ (Resume)')
    
    # --- æ¨¡å‹ç»“æ„å‚æ•° (ç”¨äºè¦†ç›– Config) ---
    parser.add_argument('--model_type', type=str, default=None)
    parser.add_argument('--hid_S', type=int, default=None)
    parser.add_argument('--hid_T', type=int, default=None)
    parser.add_argument('--N_S', type=int, default=None)
    parser.add_argument('--N_T', type=int, default=None)
    parser.add_argument('--mlp_ratio', type=float, default=None)
    parser.add_argument('--drop', type=float, default=None)
    parser.add_argument('--drop_path', type=float, default=None)
    
    # --- ä¼˜åŒ–å™¨ ---
    parser.add_argument('--opt', type=str, default='adamw')
    parser.add_argument('--lr', type=float, default=2e-4, help='å¾®è°ƒå­¦ä¹ ç‡ (å»ºè®® 2e-4)')
    parser.add_argument('--sched', type=str, default='cosine')
    parser.add_argument('--min_lr', type=float, default=1e-5)
    parser.add_argument('--warmup_epoch', type=int, default=2)
    parser.add_argument('--accumulate_grad_batches', type=int, default=1)
    parser.add_argument('--gradient_clip_val', type=float, default=1.0)
    
    # --- è®¾å¤‡ä¸ç²¾åº¦ ---
    parser.add_argument('--accelerator', type=str, default='cuda')
    parser.add_argument('--devices', type=str, default='auto')
    parser.add_argument('--precision', type=str, default='bf16-mixed')
    
    # --- æ—©åœ ---
    parser.add_argument('--early_stop_patience', type=int, default=10)
    parser.add_argument('--early_stop_monitor', type=str, default='val_score')
    parser.add_argument('--early_stop_mode', type=str, default='max')

    return parser.parse_args()

def main():
    torch.set_float32_matmul_precision('high')
    args = parse_args()
    
    # 1. å‚æ•°é¢„å¤„ç†
    # è¿‡æ»¤æ‰ None å€¼ï¼Œå‡†å¤‡ä¼ é€’ç»™ SimVPConfig
    # æ³¨æ„ï¼šSimVPConfig å¯èƒ½ä¸åŒ…å« sigma/gamma ç­‰æ–°å‚æ•°ï¼Œéœ€åç»­å•ç‹¬æ³¨å…¥
    config_kwargs = {k: v for k, v in vars(args).items() if v is not None}
    
    if 'in_shape' in config_kwargs: config_kwargs['in_shape'] = tuple(config_kwargs['in_shape'])
    
    # å¸ƒå°”å€¼å¤„ç†
    if isinstance(config_kwargs.get('use_focal'), str):
        config_kwargs['use_focal'] = config_kwargs['use_focal'].lower() == 'true'

    # å¼ºåˆ¶è®¾ç½® out_channels = num_bins
    num_bins = config_kwargs.get('num_bins', 40)
    config_kwargs['out_channels'] = num_bins 
    
    # 2. åˆå§‹åŒ– Config
    try:
        # ç§»é™¤ config ç±»ä¸æ”¯æŒçš„å‚æ•°ï¼Œé˜²æ­¢æŠ¥é”™
        valid_keys = SimVPConfig.model_fields.keys()
        safe_kwargs = {k: v for k, v in config_kwargs.items() if k in valid_keys}
        
        config = SimVPConfig(**safe_kwargs)
    except ValidationError as e:
        MLOGI(f"[ERROR] Config Validation: {e}")
        return

    l.seed_everything(config.seed)

    # 3. åˆå§‹åŒ– DataModule
    data_module = ScwdsDataModule(
        data_path=config.data_path,
        resize_shape=config.resize_shape,
        batch_size=config.batch_size,
        num_workers=config.num_workers,
        train_split=config.train_split,
        val_split=config.val_split,
        test_split=config.test_split,
        seed=config.seed
    )
    
    # 4. åˆå§‹åŒ–æ¨¡å‹ (å‚æ•°æ³¨å…¥)
    model_args = config.to_dict()
    
    # [å…³é”®] å°† Loss ç›¸å…³çš„æ–°å‚æ•°æ³¨å…¥ model_args
    # ProbabilisticSimVP éœ€è¦åœ¨ __init__ ä¸­æ¥æ”¶è¿™äº›å‚æ•°å¹¶ä¼ é€’ç»™ Loss
    model_args['num_bins'] = num_bins
    model_args['sigma'] = config_kwargs.get('sigma', 2.0)
    model_args['use_focal'] = config_kwargs.get('use_focal', True)
    model_args['gamma'] = config_kwargs.get('gamma', 2.0)
    
    MLOGI(f"[Init] Initializing ProbabilisticSimVP with: Bins={num_bins}, Sigma={model_args['sigma']}, Focal={model_args['use_focal']}")
    
    model = ProbabilisticSimVP(**model_args)

    # 5. è¿ç§»å­¦ä¹ ï¼šåŠ è½½åŸºåº§æƒé‡
    base_ckpt_path = args.ckpt_path
    if base_ckpt_path is None:
        try:
            base_ckpt_path = find_best_ckpt(args.base_ckpt_dir)
        except FileNotFoundError:
            MLOGI(f"[WARNING] æœªåœ¨ {args.base_ckpt_dir} æ‰¾åˆ° Checkpointï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
            base_ckpt_path = None
    
    if base_ckpt_path:
        MLOGI(f"ğŸš€ å¯ç”¨è¿ç§»å­¦ä¹ : è½½å…¥ SimVP åŸºåº§æƒé‡è‡ª: {base_ckpt_path}")
        try:
            ckpt = torch.load(base_ckpt_path, map_location='cpu')
            state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt
            
            new_state_dict = {}
            for k, v in state_dict.items():
                # è·³è¿‡ readout å±‚ (ç»´åº¦ä» 1 å˜ä¸º num_binsï¼Œæ— æ³•åŒ¹é…)
                if 'model.readout' in k: 
                    continue
                new_state_dict[k] = v

            model.load_state_dict(new_state_dict, strict=False)
            MLOGI("[INFO] Backbone æƒé‡åŠ è½½æˆåŠŸã€‚Readout å±‚å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹å­¦ä¹ ã€‚")

            # [å¯é€‰] å†»ç»“ Backbone 1ä¸ª Epoch (Warmup)
            # for param in model.model.parameters():
            #     param.requires_grad = False
            # for param in model.model.readout.parameters():
            #     param.requires_grad = True
            
        except Exception as e:
            MLOGI(f"[ERROR] åŠ è½½åŸºåº§æ¨¡å‹æƒé‡å¤±è´¥: {e}ã€‚å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒã€‚")

    # 6. Callbacks
    monitor_metric = config.early_stop_monitor
    monitor_mode = config.early_stop_mode

    callbacks = [
        EarlyStopping(
            monitor=monitor_metric, 
            min_delta=config.early_stop_min_delta, 
            patience=config.early_stop_patience, 
            mode=monitor_mode, 
            verbose=True
        ),
        ModelCheckpoint(
            dirpath=config.save_dir, 
            filename="prob-{epoch:02d}-{val_score:.4f}",
            monitor=monitor_metric,
            save_top_k=3, 
            mode=monitor_mode,
            save_last=True 
        ),
        LearningRateMonitor(logging_interval="step")
    ]

    logger = TensorBoardLogger(save_dir=config.save_dir, name=config.model_name, version=datetime.now().strftime("%Y%m%d-%H%M%S"))

    strategy = 'ddp_find_unused_parameters_false' if config.devices != 1 and config.accelerator == 'cuda' else 'auto'

    trainer = l.Trainer(
        max_epochs=config.max_epochs,
        default_root_dir=config.save_dir,
        precision=config.precision,
        accelerator=config.accelerator,
        devices=config.devices,
        callbacks=callbacks,
        logger=logger,
        log_every_n_steps=config.log_every_n_steps,
        val_check_interval=config.val_check_interval,
        gradient_clip_val=config.gradient_clip_val,
        strategy=strategy,
        sync_batchnorm=False, 
        enable_progress_bar=config.enable_progress_bar,
        enable_model_summary=config.enable_model_summary,
        num_sanity_val_steps=config.num_sanity_val_steps,
    )

    # 7. å¼€å§‹è®­ç»ƒ
    trainer.fit(model, datamodule=data_module, ckpt_path=args.ckpt_path)

if __name__ == "__main__":
    main()