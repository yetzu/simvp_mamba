# run/test_scwds_prob.py (åŸºäº run/test_scwds_simvp.py ä¿®æ”¹ï¼Œé€‚é…æ¦‚ç‡åˆ†ç®±æ¨¡å¼ + åŸºåº§å¯¹æ¯”)

import sys
import os
import glob
import torch
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import argparse
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
matplotlib.use('Agg')

from metai.dataset.met_dataloader_scwds import ScwdsDataModule
from metai.model.simvp.simvp_config import SimVPConfig
# [ä¿®æ”¹ç‚¹ 1] å¯¼å…¥ Probabilistic SimVP Trainer
from metai.model.simvp.prob_trainer import ProbabilisticSimVP 
from metai.model.simvp.simvp_trainer import SimVP as SimVP_Regression # å¯¼å…¥åŸ SimVP ä»¥å¤ç”¨ MetricConfig å’ŒåŠ è½½åŸºåº§

# ==========================================
# Part 0: è¾…åŠ©å·¥å…·å‡½æ•°
# ==========================================

class TeeLogger:
    """åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶çš„æ—¥å¿—ç±»"""
    def __init__(self, log_file_path):
        self.log_file = open(log_file_path, 'w', encoding='utf-8')
        self.console = sys.stdout
        
    def write(self, message):
        self.console.write(message)
        self.log_file.write(message)
        self.log_file.flush()
        
    def flush(self):
        self.console.flush()
        self.log_file.flush()
        
    def close(self):
        if self.log_file:
            self.log_file.close()
            
    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

# å…¨å±€æ—¥å¿—å¯¹è±¡ï¼Œåˆå§‹ä¸º None
_logger = None

def set_logger(log_file_path):
    global _logger
    _logger = TeeLogger(log_file_path)
    sys.stdout = _logger
    
def restore_stdout():
    global _logger
    if _logger:
        sys.stdout = _logger.console
        _logger.close()
        _logger = None

def find_best_ckpt(save_dir: str) -> str:
    # ä¼˜å…ˆæŸ¥æ‰¾ best.ckpt
    best = os.path.join(save_dir, 'best.ckpt')
    if os.path.exists(best): return best
    
    # å…¶æ¬¡æŸ¥æ‰¾ last.ckpt
    last = os.path.join(save_dir, 'last.ckpt')
    if os.path.exists(last): return last
    
    # æœ€åæŸ¥æ‰¾æ‰€æœ‰ checkpoint æ–‡ä»¶ï¼Œè¿”å›æœ€æ–°çš„
    cpts = sorted(glob.glob(os.path.join(save_dir, '*.ckpt')))
    if len(cpts) == 0:
        raise FileNotFoundError(f'No checkpoint found in {save_dir}')
    return cpts[-1]

def get_checkpoint_info(ckpt_path: str):
    """ä» checkpoint æ–‡ä»¶ä¸­æå–è®­ç»ƒå…³é”®ä¿¡æ¯"""
    try:
        # ä½¿ç”¨ SimVP_Regression çš„ load_from_checkpoint é€»è¾‘æ¥åŠ è½½ä¿¡æ¯
        ckpt = torch.load(ckpt_path, map_location='cpu')
        epoch = ckpt.get('epoch', None)
        global_step = ckpt.get('global_step', None)
        hparams = ckpt.get('hyper_parameters', {})
        return {
            'epoch': epoch,
            'global_step': global_step,
            'hparams': hparams,
            'ckpt_name': os.path.basename(ckpt_path)
        }
    except Exception as e:
        return {'error': str(e)}

def print_checkpoint_info(ckpt_info: dict):
    """æ‰“å° checkpoint ä¿¡æ¯"""
    if 'error' in ckpt_info:
        print(f"[WARNING] æ— æ³•è¯»å– checkpoint ä¿¡æ¯: {ckpt_info['error']}")
        return
    
    print("=" * 80)
    print(f"[INFO] Loaded Checkpoint: {ckpt_info['ckpt_name']}")
    print(f"  Epoch: {ckpt_info.get('epoch', 'N/A')}")
    print(f"  Global Step: {ckpt_info.get('global_step', 'N/A')}")
    
    hparams = ckpt_info.get('hparams', {})
    if hparams:
        print(f"  Model Type: {hparams.get('model_type', 'N/A')}")
        print(f"  In Shape: {hparams.get('in_shape', 'N/A')}")
        print(f"  Out Seq Length: {hparams.get('aft_seq_length', 'N/A')}")
        print(f"  Prob Bins: {hparams.get('num_bins', 'N/A')}") # [ä¿®æ”¹ç‚¹ 0] æ‰“å° num_bins
    print("=" * 80)

# ==========================================
# Part 1: å…¨å±€è¯„åˆ†é…ç½® (Metric Configuration) - å¤ç”¨ SimVP Trainer çš„å¸¸é‡
# ==========================================
# æ³¨æ„ï¼šæˆ‘ä»¬å¤ç”¨åŸ SimVP Trainer çš„ MetricConfigï¼Œå› ä¸ºå®ƒä¸æ¯”èµ›è§„åˆ™ä¸€è‡´
class MetricConfig:
    MM_MAX = 30.0
    THRESHOLD_NOISE = 0.05 
    LEVEL_EDGES = np.array([0.1, 1.0, 2.0, 5.0, 8.0, np.inf], dtype=np.float32)
    _raw_level_weights = np.array([0.1, 0.1, 0.2, 0.25,  0.35], dtype=np.float32)
    LEVEL_WEIGHTS = _raw_level_weights / _raw_level_weights.sum()

    TIME_WEIGHTS_DICT = {
        0: 0.0075, 1: 0.02, 2: 0.03, 3: 0.04, 4: 0.05,
        5: 0.06, 6: 0.07, 7: 0.08, 8: 0.09, 9: 0.1,
        10: 0.09, 11: 0.08, 12: 0.07, 13: 0.06, 14: 0.05,
        15: 0.04, 16: 0.03, 17: 0.02, 18: 0.0075, 19: 0.005
    }

    @staticmethod
    def get_time_weights(T):
        if T == 20:
            return np.array([MetricConfig.TIME_WEIGHTS_DICT[t] for t in range(T)], dtype=np.float32)
        else:
            print(f"[WARN] T={T}, expected 20. Using uniform time weights.")
            return np.ones(T, dtype=np.float32) / T

# ==========================================
# Part 2: æ ¸å¿ƒç»Ÿè®¡è®¡ç®— (Core Metrics)
# ==========================================
def calc_seq_metrics(true_seq, pred_seq, verbose=True):
    T, H, W = true_seq.shape
    
    pred_clean = pred_seq.copy()
    pred_clean[pred_clean < (MetricConfig.THRESHOLD_NOISE / MetricConfig.MM_MAX)] = 0.0
    
    time_weights = MetricConfig.get_time_weights(T)
    score_k_list = []
    
    tru_mm_seq = np.clip(true_seq, 0.0, None) * MetricConfig.MM_MAX
    prd_mm_seq = np.clip(pred_clean, 0.0, None) * MetricConfig.MM_MAX
    
    ts_mean_levels = np.zeros(len(MetricConfig.LEVEL_WEIGHTS))
    mae_mm_mean_levels = np.zeros(len(MetricConfig.LEVEL_WEIGHTS))
    corr_sum = 0.0

    if verbose:
        print(f"True Stats (mm): Max={np.max(tru_mm_seq):.2f}, Mean={np.mean(tru_mm_seq):.2f}")
        print(f"Pred Stats (mm): Max={np.max(prd_mm_seq):.2f}, Mean={np.mean(prd_mm_seq):.2f}")
        print("-" * 90)
        print(f"{'T':<3} | {'Corr(R)':<9} | {'TS_w_sum':<9} | {'Score_k':<9} | {'W_time':<8}")
        print("-" * 90)

    for t in range(T):
        tru_frame = tru_mm_seq[t].reshape(-1)
        prd_frame = prd_mm_seq[t].reshape(-1)
        abs_err = np.abs(prd_frame - tru_frame)

        mask_valid_corr = (tru_frame > 0) | (prd_frame > 0)
        if mask_valid_corr.sum() > 1:
            t_valid = tru_frame[mask_valid_corr]
            p_valid = prd_frame[mask_valid_corr]
            numerator = np.sum((t_valid - t_valid.mean()) * (p_valid - p_valid.mean()))
            denom = np.sqrt(np.sum((t_valid - t_valid.mean())**2) * np.sum((p_valid - p_valid.mean())**2))
            R_k = numerator / (denom + 1e-8)
        else:
            R_k = 0.0
        
        R_k = float(np.clip(R_k, -1.0, 1.0))
        corr_sum += R_k
        term_corr = np.sqrt(np.exp(R_k - 1.0))

        weighted_sum_metrics = 0.0
        for i in range(len(MetricConfig.LEVEL_WEIGHTS)):
            low = MetricConfig.LEVEL_EDGES[i]
            high = MetricConfig.LEVEL_EDGES[i+1]
            w_i = MetricConfig.LEVEL_WEIGHTS[i]

            tru_bin = (tru_frame >= low) & (tru_frame < high)
            prd_bin = (prd_frame >= low) & (prd_frame < high)
            
            tp = np.logical_and(tru_bin, prd_bin).sum()
            fn = np.logical_and(tru_bin, ~prd_bin).sum()
            fp = np.logical_and(~tru_bin, prd_bin).sum()
            denom_ts = tp + fn + fp
            ts_val = (tp / denom_ts) if denom_ts > 0 else 1e-6
            
            mask_eval = tru_bin | prd_bin
            mae_val = np.mean(abs_err[mask_eval]) if mask_eval.sum() > 0 else 0.0
            
            ts_mean_levels[i] += ts_val / T
            mae_mm_mean_levels[i] += mae_val / T
            
            term_mae = np.sqrt(np.exp(-mae_val / 100.0))
            weighted_sum_metrics += w_i * ts_val * term_mae

        Score_k = term_corr * weighted_sum_metrics
        score_k_list.append(Score_k)

        if verbose:
            print(f"{t:<3} | {R_k:<9.4f} | {weighted_sum_metrics:<9.4f} | {Score_k:<9.4f} | {time_weights[t]:<8.4f}")

    score_k_arr = np.array(score_k_list)
    final_score = np.sum(score_k_arr * time_weights)
    
    print("-" * 90)
    ts_str = ", ".join([f"{v:.3f}" for v in ts_mean_levels])
    mae_str = ", ".join([f"{v:.3f}" for v in mae_mm_mean_levels])
    
    print(f"[METRIC] TS_mean  (Levels): {ts_str}")
    print(f"[METRIC] MAE_mean (Levels): {mae_str}")
    print(f"[METRIC] Corr_mean: {corr_sum / T:.4f}")
    print(f"[METRIC] Final_Weighted_Score: {final_score:.6f}")
    print(f"[METRIC] Score_per_t: {', '.join([f'{s:.3f}' for s in score_k_arr])}")
    print("-" * 90)
    
    return {
        "final_score": final_score,
        "score_per_frame": score_k_arr,
        "pred_clean": pred_clean
    }

# ==========================================
# Part 3: ç»˜å›¾åŠŸèƒ½ (Visualization)
# ==========================================

def create_precipitation_cmap():
    """
    åˆ›å»ºè‡ªå®šä¹‰é™æ°´è‰²æ ‡
    åŒºé—´: 0.01 <= r < 0.1, 0.1 <= r < 1, ..., r >= 8
    0 å€¼ (åŠ < 0.01) æ˜¾ç¤ºä¸ºç™½è‰²
    """
    hex_colors = [
        '#9CF48D',  # 0.01 <= r < 0.1 (æµ…ç»¿)
        '#3CB73A',  # 0.1 <= r < 1 (ä¸­ç»¿)
        '#63B7FF',  # 1 <= r < 2 (æµ…è“)
        '#0200F9',  # 2 <= r < 5 (æ·±è“)
        '#EE00F0',  # 5 <= r < 8 (ç´«çº¢)
        '#9F0000'   # r >= 8 (æ·±çº¢)
    ]
    
    cmap = mcolors.ListedColormap(hex_colors)
    cmap.set_bad('white')
    cmap.set_under('white')
    
    # è¾¹ç•Œè®¾ç½®ï¼šèµ·å§‹å€¼è®¾ä¸º0.01ï¼Œç¡®ä¿0å€¼è½å…¥underåŒºåŸŸï¼ˆç™½è‰²ï¼‰
    bounds = [0.01, 0.1, 1, 2, 5, 8, 100]
    norm = mcolors.BoundaryNorm(boundaries=bounds, ncolors=len(hex_colors))
    
    return cmap, norm

def plot_seq_visualization(obs_seq, true_seq, pred_seq, base_seq, scores, out_path, vmax=1.0):
    """
    ç»˜åˆ¶ Obs, GT, Base Pred, Prob Pred, Diff å¯¹æ¯”å›¾
    [ä¿®æ”¹] å¢åŠ  base_seq å‚æ•°ï¼Œå¦‚æœé Noneï¼Œåˆ™ç»˜åˆ¶ 5 è¡Œï¼Œå¦åˆ™ç»˜åˆ¶ 4 è¡Œ
    """
    T = true_seq.shape[0] # T_out = 20
    rows = 5 if base_seq is not None else 4
    cols = T
    
    precip_cmap, precip_norm = create_precipitation_cmap()
    
    obs_mm = obs_seq * MetricConfig.MM_MAX
    true_mm = true_seq * MetricConfig.MM_MAX
    pred_mm = pred_seq * MetricConfig.MM_MAX
    
    # å¢åŠ é«˜åº¦ä»¥å®¹çº³åº•éƒ¨ Legend
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5 + 1.0))
    if T == 1: axes = axes[:, np.newaxis]
    
    fig.subplots_adjust(bottom=0.12, top=0.95, left=0.02, right=0.98, wspace=0.1, hspace=0.3)

    # è¾…åŠ©å‡½æ•°ï¼šç»Ÿä¸€è®¾ç½®è¾¹æ¡†æ ·å¼
    def setup_ax_border(ax, show_ylabel=False, ylabel_text=""):
        # å…³é”®ä¿®æ”¹ï¼šä¸è¦ä½¿ç”¨ ax.axis('off')ï¼Œå¦åˆ™è¾¹æ¡†ä¼šè¢«éšè—
        # è€Œæ˜¯éšè—åˆ»åº¦
        ax.set_xticks([])
        ax.set_yticks([])
        
        # æ˜¾å¼å¼€å¯è¾¹æ¡†
        for spine in ax.spines.values():
            spine.set_visible(True)
            spine.set_edgecolor('black')
            spine.set_linewidth(0.5)
            
        if show_ylabel:
            ax.set_ylabel(ylabel_text, fontsize=8)
            # ç¡®ä¿ Y è½´ Label æ˜¾ç¤º
            ax.yaxis.label.set_visible(True)

    input_len = obs_mm.shape[0]

    for t in range(T):
        # 1. Obs (Input)
        ax = axes[0, t]
        if t < input_len:
            ax.imshow(obs_mm[t], cmap=precip_cmap, norm=precip_norm)
            # [ä¿®æ”¹] æ ‡ç­¾é€»è¾‘ï¼šå€’åºæ˜¾ç¤º T-x
            time_idx = input_len - 1 - t
            ax.set_title(f'T-{time_idx}', fontsize=6)
        else:
            ax.imshow(np.zeros_like(true_mm[0]), cmap=precip_cmap, norm=precip_norm)
        
        setup_ax_border(ax, show_ylabel=(t==0), ylabel_text='Obs')

        # 2. GT (Target)
        ax = axes[1, t]
        ax.imshow(true_mm[t], cmap=precip_cmap, norm=precip_norm)
        ax.set_title(f'T+{t+1}', fontsize=6)
        setup_ax_border(ax, show_ylabel=(t==0), ylabel_text='GT')

        # 3. Base Pred (Optional)
        current_row = 2
        if base_seq is not None:
            base_mm = base_seq * MetricConfig.MM_MAX
            ax = axes[current_row, t]
            ax.imshow(base_mm[t], cmap=precip_cmap, norm=precip_norm)
            setup_ax_border(ax, show_ylabel=(t==0), ylabel_text='Base')
            current_row += 1

        # 4. Prob Pred
        ax = axes[current_row, t]
        ax.imshow(pred_mm[t], cmap=precip_cmap, norm=precip_norm)
        setup_ax_border(ax, show_ylabel=(t==0), ylabel_text='Prob')
        current_row += 1
        
        # 5. Diff (GT - Prob Pred)
        ax = axes[current_row, t]
        diff = true_mm[t] - pred_mm[t]
        ax.imshow(diff, cmap='bwr', vmin=-30, vmax=30)
        setup_ax_border(ax, show_ylabel=(t==0), ylabel_text='Diff')

    # --- Legend 1: Precipitation (å·¦ä¾§) ---
    cbar_ax_precip = fig.add_axes([0.20, 0.05, 0.25, 0.015])
    sm_precip = plt.cm.ScalarMappable(cmap=precip_cmap, norm=precip_norm)
    sm_precip.set_array([])
    cbar_p = fig.colorbar(sm_precip, cax=cbar_ax_precip, orientation='horizontal', spacing='uniform')
    cbar_p.set_ticks([0.1, 1, 2, 5, 8])
    cbar_p.set_ticklabels(['0.1', '1', '2', '5', '8'])
    cbar_p.set_label('Precipitation (mm)', fontsize=8)
    cbar_p.ax.tick_params(labelsize=7)
    
    # --- Legend 2: Diff (å³ä¾§) ---
    cbar_ax_diff = fig.add_axes([0.55, 0.05, 0.25, 0.015])
    sm_diff = plt.cm.ScalarMappable(cmap='bwr', norm=plt.Normalize(vmin=-30, vmax=30))
    sm_diff.set_array([])
    cbar_d = fig.colorbar(sm_diff, cax=cbar_ax_diff, orientation='horizontal')
    cbar_d.set_ticks([-30, -15, 0, 15, 30])
    cbar_d.set_label('Difference (mm)', fontsize=8)
    cbar_d.ax.tick_params(labelsize=7)

    print(f'[INFO] Saving Plot to {out_path}')
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    plt.savefig(out_path, dpi=150)
    plt.close(fig)

# ==========================================
# Part 4: ä¸»å…¥å£å‡½æ•° (Wrapper)
# ==========================================
def render(obs_seq, true_seq, pred_seq, base_seq, out_path: str, vmax: float = 1.0):
    # 1. æ•°æ®æ ¼å¼ç»Ÿä¸€ (è½¬ Numpy & æå–é€šé“)
    def to_numpy_ch(x, ch=0):
        if isinstance(x, torch.Tensor): x = x.detach().cpu().numpy()
        # è¾“å…¥ç»´åº¦ (T, C, H, W), é€šé“ 0 æ˜¯é›·è¾¾/é™æ°´æ ‡ç­¾
        if x.ndim == 4: x = x[:, ch] 
        return x

    obs = to_numpy_ch(obs_seq) # å– ch=0 (Radar/Label)
    tru = to_numpy_ch(true_seq)
    prd = to_numpy_ch(pred_seq)
    
    # å¤„ç†åŸºåº§é¢„æµ‹
    base = to_numpy_ch(base_seq) if base_seq is not None else None
    
    print(f"Processing: {os.path.basename(out_path)}")
    
    # 2. è°ƒç”¨ç»Ÿè®¡æ¨¡å— (åªè®¡ç®— Prob æ¨¡å‹çš„æŒ‡æ ‡)
    metrics_res = calc_seq_metrics(tru, prd, verbose=True)
    
    final_score = metrics_res['final_score']
    
    # 3. è°ƒç”¨ç»˜å›¾æ¨¡å— (ä¼ å…¥ base_seq)
    plot_seq_visualization(obs, tru, metrics_res['pred_clean'], base, metrics_res['score_per_frame'], out_path, vmax=vmax)
    
    return final_score

def parse_args():
    parser = argparse.ArgumentParser(description='Test SCWDS Probabilistic SimVP Model')
    parser.add_argument('--data_path', type=str, default='data/samples.jsonl')
    parser.add_argument('--in_shape', type=int, nargs=4, default=[10, 54, 256, 256],
                        help='Input shape (T, C, H, W). Default: [10, 54, 256, 256]')
    parser.add_argument('--aft_seq_length', type=int, default=20,
                        help='Output sequence length. Default: 20')
    parser.add_argument('--save_dir', type=str, default='./output/prob_simvp')
    parser.add_argument('--num_samples', type=int, default=10)
    parser.add_argument('--accelerator', type=str, default='cuda')
    
    # [æ–°å¢] åˆ†ç®±å‚æ•°
    parser.add_argument('--num_bins', type=int, default=64, help='æ¦‚ç‡åˆ†ç®±çš„æ•°é‡')
    
    # [æ–°å¢] åŸºåº§æ¨¡å‹è·¯å¾„å‚æ•°
    parser.add_argument('--base_ckpt_dir', type=str, default='./output/simvp', help='åŸºåº§ SimVP æ¨¡å‹çš„ Checkpoint ç›®å½•')
    
    return parser.parse_args()

def main():
    args = parse_args()
    device = torch.device(args.accelerator if torch.cuda.is_available() else 'cpu')
    
    # 1. Config & Model
    config = SimVPConfig(
        data_path=args.data_path,
        in_shape=tuple(args.in_shape),
        save_dir=args.save_dir,
        aft_seq_length=args.aft_seq_length,
        out_channels=args.num_bins, # è®¾ç½® out_channels
        num_bins=args.num_bins       # æ³¨å…¥ num_bins
    )
    resize_shape = (config.in_shape[2], config.in_shape[3])
    
    # åŠ è½½ Prob æ¨¡å‹ Checkpoint
    ckpt_path = find_best_ckpt(config.save_dir)
    ckpt_info = get_checkpoint_info(ckpt_path)
    
    epoch = ckpt_info.get('epoch', 0)
    
    out_dir = os.path.join(config.save_dir, f'vis_prob_{epoch:02d}')
    os.makedirs(out_dir, exist_ok=True)
    
    log_file_path = os.path.join(out_dir, 'log.txt')
    set_logger(log_file_path)
    
    print(f"[INFO] Starting Probabilistic Test on {device}")
    print_checkpoint_info(ckpt_info)
    
    # [ä¿®æ”¹ç‚¹ 2] ä½¿ç”¨ ProbabilisticSimVP åŠ è½½æ¨¡å‹ï¼Œå¹¶ä¼ å…¥ num_bins å‚æ•°
    model = ProbabilisticSimVP.load_from_checkpoint(ckpt_path, num_bins=config.num_bins)
    model.eval().to(device)
    
    # [æ–°å¢] åŠ è½½åŸºåº§ SimVP æ¨¡å‹ (Regression)
    base_model = None
    if args.base_ckpt_dir:
        try:
            base_ckpt_path = find_best_ckpt(args.base_ckpt_dir)
            print(f"[INFO] Loading Base Model from: {base_ckpt_path}")
            base_model = SimVP_Regression.load_from_checkpoint(base_ckpt_path, map_location=device)
            base_model.eval().to(device)
        except Exception as e:
            print(f"[WARNING] æ— æ³•åŠ è½½åŸºåº§æ¨¡å‹ï¼Œå°†è·³è¿‡åŸºåº§å¯¹æ¯”: {e}")
            base_model = None

    # 2. Data
    data_module = ScwdsDataModule(
        data_path=config.data_path,
        resize_shape=resize_shape,
        batch_size=1,
        num_workers=4
    )
    data_module.setup('test')
    test_loader = data_module.test_dataloader()
    
    scores = []
    
    with torch.no_grad():
        for bidx, batch in enumerate(test_loader):
            # [å…³é”®ä¿®å¤] å°† Tensor ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ (GPU)
            metadata, x, y, target_mask, input_mask = batch
            
            x = x.to(device)
            y = y.to(device)
            target_mask = target_mask.to(device)
            input_mask = input_mask.to(device)
            
            # é‡æ–°æ‰“åŒ…
            batch_on_device = (metadata, x, y, target_mask, input_mask)
            
            # 1. è¿è¡Œ Prob æ¨¡å‹æ¨ç†
            # ProbabilisticSimVP.test_step å·²æ‰§è¡Œ Argmax è§£ç å’Œå½’ä¸€åŒ–
            outputs = model.test_step(batch_on_device, bidx)
            
            # 2. [æ–°å¢] è¿è¡Œ Base æ¨¡å‹æ¨ç†
            base_preds = None
            if base_model is not None:
                # SimVP forward è¿”å› raw logits [B, T, C, H, W]
                
                # [ğŸ”¥ å…³é”®ä¿®å¤] æ˜¾å¼è°ƒç”¨ Resizeï¼Œå°† 301x301 -> 256x256
                # å¦åˆ™æ¨¡å‹å†…éƒ¨ Skip Connection ä¼šå‡ºç° 304 vs 301 çš„å°ºå¯¸ä¸åŒ¹é…
                x_base = base_model._interpolate_batch_gpu(x, mode='max_pool')
                
                base_logits = base_model(x_base)
                base_sigmoid = torch.sigmoid(base_logits)
                base_clamped = torch.clamp(base_sigmoid, 0.0, 1.0)
                # æå– numpy æ•°æ® [T, C, H, W]
                base_preds = base_clamped[0].cpu().numpy()
            
            save_path = os.path.join(out_dir, f'sample_{bidx:03d}.png')
            
            # outputs['preds'] æ˜¯ Argmax è§£ç åçš„å½’ä¸€åŒ–æ•°å€¼ [T, 1, H, W]
            # ä¼ å…¥ base_preds è¿›è¡Œå¯¹æ¯”æ¸²æŸ“
            s = render(outputs['inputs'], outputs['trues'], outputs['preds'], base_preds, save_path)
            scores.append(s)
            
            if bidx >= args.num_samples - 1:
                break
    
    if len(scores) > 0:
        print(f"\n[FINAL] Average Probabilistic Score ({len(scores)} samples): {np.mean(scores):.6f}")
    
    restore_stdout()

if __name__ == '__main__':
    try:
        main()
    finally:
        restore_stdout()