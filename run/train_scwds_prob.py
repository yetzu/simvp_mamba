# train_scwds_prob.py (æ¦‚ç‡åˆ†ç®± SimVP-Mamba è¿ç§»å­¦ä¹ è®­ç»ƒè„šæœ¬)

import sys
import os
import glob
from datetime import datetime
import argparse
import ast
from pydantic import ValidationError

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import lightning as l
from lightning.pytorch.loggers import TensorBoardLogger
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor

from metai.dataset.met_dataloader_scwds import ScwdsDataModule
from metai.model.simvp.simvp_config import SimVPConfig
# å¯¼å…¥ Probabilistic Trainer å’Œ SimVP åŸºåº§æ¨¡å‹ï¼ˆç”¨äºåŠ è½½æƒé‡ï¼‰
from metai.model.simvp.prob_trainer import ProbabilisticSimVP # å‡è®¾ProbabilisticSimVPä½äºæ­¤
from metai.model.simvp.simvp_trainer import SimVP # å‡è®¾SimVP (å›å½’ç‰ˆ)ä½äºæ­¤
from metai.utils import MLOGI

def find_best_ckpt(save_dir: str) -> str:
    """æŸ¥æ‰¾æœ€ä¼˜æˆ–æœ€æ–°çš„ Checkpoint æ–‡ä»¶ï¼Œä¼˜å…ˆ best.ckpt"""
    # ä¼˜å…ˆæŸ¥æ‰¾ best.ckpt
    best = os.path.join(save_dir, 'best.ckpt')
    if os.path.exists(best): return best
    
    # å…¶æ¬¡æŸ¥æ‰¾ last.ckpt
    last = os.path.join(save_dir, 'last.ckpt')
    if os.path.exists(last): return last
    
    # æœ€åæŸ¥æ‰¾æ‰€æœ‰ checkpoint æ–‡ä»¶ï¼Œè¿”å›æœ€æ–°çš„
    cpts = sorted(glob.glob(os.path.join(save_dir, '*.ckpt')))
    if len(cpts) > 0: return cpts[-1]
        
    raise FileNotFoundError(f'No checkpoint found in {save_dir}')

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Train SCWDS Probabilistic SimVP Model (Transfer Learning)')
    
    # åŸºç¡€è·¯å¾„ä¸æ•°æ®å‚æ•°
    parser.add_argument('--data_path', type=str, default='data/samples.jsonl', help='Path to training data')
    parser.add_argument('--save_dir', type=str, default='./output/prob_simvp', help='Output directory for Probabilistic Model')
    parser.add_argument('--in_shape', type=int, nargs=4, default=None) 
    parser.add_argument('--batch_size', type=int, default=None)
    parser.add_argument('--max_epochs', type=int, default=30, help='å¾®è°ƒçš„æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--num_workers', type=int, default=None)
    parser.add_argument('--aft_seq_length', type=int, default=None)

    # [æ ¸å¿ƒå‚æ•°] æ¦‚ç‡åˆ†ç®±å‚æ•°
    parser.add_argument('--num_bins', type=int, default=64, help='æ¦‚ç‡åˆ†ç®±çš„æ•°é‡')
    
    # [è¿ç§»å­¦ä¹ å‚æ•°]
    parser.add_argument('--base_ckpt_dir', type=str, required=True, help='SimVPåŸºåº§æ¨¡å‹(å›å½’ç‰ˆ)çš„ä¿å­˜ç›®å½•ï¼Œå°†è‡ªåŠ¨æŸ¥æ‰¾ best.ckpt')
    parser.add_argument('--ckpt_path', type=str, default=None, help='å¦‚æœæŒ‡å®šï¼Œåˆ™ç›´æ¥åŠ è½½è¯¥è·¯å¾„çš„æ¨¡å‹ä½œä¸ºåˆå§‹æƒé‡')
    
    # æ¨¡å‹ç»“æ„å‚æ•°
    parser.add_argument('--model_type', type=str, default=None)
    parser.add_argument('--hid_S', type=int, default=None)
    parser.add_argument('--hid_T', type=int, default=None)
    parser.add_argument('--N_S', type=int, default=None)
    parser.add_argument('--N_T', type=int, default=None)
    parser.add_argument('--mlp_ratio', type=float, default=None)
    parser.add_argument('--drop', type=float, default=None)
    parser.add_argument('--drop_path', type=float, default=None)
    
    # ä¼˜åŒ–å™¨
    parser.add_argument('--opt', type=str, default='adamw')
    parser.add_argument('--lr', type=float, default=1e-4, help='å¾®è°ƒé˜¶æ®µçš„æ¨èå­¦ä¹ ç‡ (1e-4 ~ 1e-5)')
    parser.add_argument('--sched', type=str, default='cosine')
    parser.add_argument('--min_lr', type=float, default=1e-5)
    parser.add_argument('--warmup_epoch', type=int, default=2)
    parser.add_argument('--accumulate_grad_batches', type=int, default=1)
    parser.add_argument('--gradient_clip_val', type=float, default=1.0)
    parser.add_argument('--gradient_clip_algorithm', type=str, default='norm')
    
    # è®¾å¤‡ä¸ç²¾åº¦
    parser.add_argument('--accelerator', type=str, default='cuda')
    parser.add_argument('--devices', type=str, default='auto')
    parser.add_argument('--precision', type=str, default='bf16-mixed')
    
    # æ—©åœå‚æ•°
    parser.add_argument('--early_stop_patience', type=int, default=10)
    parser.add_argument('--early_stop_monitor', type=str, default='val_score')
    parser.add_argument('--early_stop_mode', type=str, default='max')

    return parser.parse_args()

def main():
    torch.set_float32_matmul_precision('high')
    args = parse_args()
    
    config_kwargs = {k: v for k, v in vars(args).items() if v is not None}
    
    if 'in_shape' in config_kwargs: config_kwargs['in_shape'] = tuple(config_kwargs['in_shape'])
    
    # [æ ¸å¿ƒè®¾ç½®] å¼ºåˆ¶è®¾ç½® out_channels ä¸º num_bins
    num_bins = config_kwargs.get('num_bins', 64)
    config_kwargs['out_channels'] = num_bins 
    
    # ... (è®¾å¤‡å’Œå¸ƒå°”å€¼å¤„ç†ï¼Œä¿æŒä¸ SimVP è„šæœ¬ä¸€è‡´) ...

    try:
        config = SimVPConfig(**config_kwargs)
    except ValidationError as e:
        MLOGI(f"[ERROR] Config Validation: {e}")
        return

    l.seed_everything(config.seed)

    data_module = ScwdsDataModule(
        data_path=config.data_path,
        resize_shape=config.resize_shape,
        batch_size=config.batch_size,
        num_workers=config.num_workers,
        train_split=config.train_split,
        val_split=config.val_split,
        test_split=config.test_split,
        seed=config.seed
    )
    
    # 1. åˆå§‹åŒ–æ¦‚ç‡åˆ†ç®±æ¨¡å‹
    model_args = config.to_dict()
    model = ProbabilisticSimVP(**model_args)

    # 2. [æ ¸å¿ƒ] æŸ¥æ‰¾å¹¶åŠ è½½åŸºåº§æ¨¡å‹æƒé‡ (Transfer Learning)
    base_ckpt_path = args.ckpt_path
    if base_ckpt_path is None:
        try:
            base_ckpt_path = find_best_ckpt(args.base_ckpt_dir)
        except FileNotFoundError:
            MLOGI(f"[WARNING] æœªåœ¨ {args.base_ckpt_dir} æ‰¾åˆ° Checkpointï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
            base_ckpt_path = None
    
    if base_ckpt_path:
        MLOGI(f"ğŸš€ å¯ç”¨è¿ç§»å­¦ä¹ : è½½å…¥ SimVP åŸºåº§æƒé‡è‡ª: {base_ckpt_path}")
        try:
            ckpt = torch.load(base_ckpt_path, map_location='cpu')
            state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt
            
            new_state_dict = {}
            for k, v in state_dict.items():
                # ä»…è·³è¿‡ readout å±‚ï¼Œå…¶ä»–å±‚å‡åŠ è½½
                if 'model.readout' in k: 
                    continue
                new_state_dict[k] = v

            # è½½å…¥é™¤ readout ä¹‹å¤–çš„æ‰€æœ‰æƒé‡ (strict=False å…è®¸ readout å±‚ç¼ºå¤±)
            model.load_state_dict(new_state_dict, strict=False)
            MLOGI("[INFO] Backbone (Encoder+Mamba+Decoder) æƒé‡åŠ è½½æˆåŠŸã€‚Readout å±‚å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹å­¦ä¹ ã€‚")

        except Exception as e:
            MLOGI(f"[ERROR] åŠ è½½åŸºåº§æ¨¡å‹æƒé‡å¤±è´¥: {e}ã€‚å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒã€‚")

    # 3. Callbacks å’Œ Trainer åˆå§‹åŒ–
    monitor_metric = config.early_stop_monitor
    monitor_mode = config.early_stop_mode

    callbacks = [
        # æ—©åœ
        EarlyStopping(
            monitor=monitor_metric, 
            min_delta=config.early_stop_min_delta, 
            patience=config.early_stop_patience, 
            mode=monitor_mode, 
            verbose=True
        ),
        
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        ModelCheckpoint(
            dirpath=config.save_dir, 
            filename="prob-{epoch:02d}-{val_score:.4f}",
            monitor=monitor_metric,
            save_top_k=3, 
            mode=monitor_mode,
            save_last=True # æ€»æ˜¯ä¿å­˜ last.ckpt ç”¨äºæ–­ç‚¹ç»­è®­
        ),
        
        LearningRateMonitor(logging_interval="step")
    ]

    logger = TensorBoardLogger(save_dir=config.save_dir, name=config.model_name, version=datetime.now().strftime("%Y%m%d-%H%M%S"))

    # DDP Strategy
    strategy = 'ddp_find_unused_parameters_false' if config.devices != 1 and config.accelerator == 'cuda' else 'auto'

    trainer = l.Trainer(
        max_epochs=config.max_epochs,
        default_root_dir=config.save_dir,
        precision=config.precision,
        accelerator=config.accelerator,
        devices=config.devices,
        callbacks=callbacks,
        logger=logger,
        log_every_n_steps=config.log_every_n_steps,
        val_check_interval=config.val_check_interval,
        gradient_clip_val=config.gradient_clip_val,
        gradient_clip_algorithm=config.gradient_clip_algorithm,
        accumulate_grad_batches=config.accumulate_grad_batches,
        strategy=strategy,
        sync_batchnorm=False, 
        enable_progress_bar=config.enable_progress_bar,
        enable_model_summary=config.enable_model_summary,
        num_sanity_val_steps=config.num_sanity_val_steps,
    )

    MLOGI(f"Starting Probabilistic Training: Model={config.model_type}, Bins={config.num_bins}")
    MLOGI(f"  Transfer Learning Source: {base_ckpt_path or 'None'}")
    
    # 4. å¯åŠ¨è®­ç»ƒ
    trainer.fit(model, datamodule=data_module, ckpt_path=args.ckpt_path)

if __name__ == "__main__":
    main()