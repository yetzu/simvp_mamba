# run/train_scwds_prob.py
# ==============================================================================
# åŠŸèƒ½: æ¦‚ç‡åˆ†ç®± SimVP-Mamba è¿ç§»å­¦ä¹ è®­ç»ƒè„šæœ¬ (Probabilistic Transfer Learning)
# ç‰¹æ€§:
#   1. é›†æˆ Focal Loss + Gaussian Soft Label ä»¥è§£å†³åºæ•°ä¸¢å¤±å’Œè™šè­¦é—®é¢˜ã€‚
#   2. å®ç°ä¸¤é˜¶æ®µå¾®è°ƒ (Two-Stage Finetuning): å…ˆå†»ç»“ Backbone è®­ç»ƒ Headï¼Œå†å…¨ç½‘å¾®è°ƒã€‚
#   3. æ”¯æŒè‡ªåŠ¨æŸ¥æ‰¾åŸºåº§æ¨¡å‹ Checkpoint è¿›è¡Œçƒ­å¯åŠ¨ã€‚
# ==============================================================================

import sys
import os
import glob
from datetime import datetime
import argparse
import ast
from pydantic import ValidationError

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import lightning as l
from lightning.pytorch.loggers import TensorBoardLogger
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor, BaseFinetuning

from metai.dataset.met_dataloader_scwds import ScwdsDataModule
from metai.model.simvp.simvp_config import SimVPConfig
from metai.model.simvp.prob_trainer import ProbabilisticSimVP 
from metai.utils import MLOGI

# ==============================================================================
# è‡ªå®šä¹‰å›è°ƒï¼šæ¦‚ç‡æ¨¡å‹å¾®è°ƒç­–ç•¥ (Probabilistic Finetuning Callback)
# ==============================================================================
class ProbabilisticFinetuning(BaseFinetuning):
    """
    å®ç°â€œå…ˆå†»ç»“åè§£å†»â€çš„è¿ç§»å­¦ä¹ ç­–ç•¥ã€‚
    
    é˜¶æ®µ 1 (Warmup): 
        - å†»ç»“ SimVP Backbone (Encoder, Translator/Mamba, Decoder)ã€‚
        - ä»…è®­ç»ƒ Readout å±‚ (C_hid -> num_bins)ï¼Œä½¿å…¶é€‚åº”åˆ†ç±»ä»»åŠ¡çš„è¾“å‡ºåˆ†å¸ƒã€‚
        
    é˜¶æ®µ 2 (Finetuning):
        - åœ¨ `unfreeze_at_epoch` è½®æ¬¡è§£å†»æ‰€æœ‰å±‚ã€‚
        - è¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼Œä¼˜åŒ–æ•´ä½“ç‰¹å¾æå–èƒ½åŠ›ã€‚
    """
    def __init__(self, unfreeze_at_epoch=2):
        super().__init__()
        self.unfreeze_at_epoch = unfreeze_at_epoch

    def freeze_before_training(self, pl_module):
        # å†»ç»“é™¤ readout å¤–çš„æ‰€æœ‰å±‚
        # æ³¨æ„ï¼štrain_bn=False æ„å‘³ç€ BN å±‚çš„ç»Ÿè®¡é‡(running_mean/var)ä¸ä¼šæ›´æ–°ï¼Œä½† gamma/beta ä¼šè¢«å†»ç»“
        self.freeze(pl_module.model.enc, train_bn=False)
        self.freeze(pl_module.model.hid, train_bn=False)
        self.freeze(pl_module.model.dec, train_bn=False)
        
        # ç¡®ä¿ readout æ˜¯è§£å†»çš„ (è¿™æ˜¯æˆ‘ä»¬è¦ä»å¤´è®­ç»ƒçš„å±‚)
        self.make_trainable(pl_module.model.readout)
        MLOGI("ğŸ¥¶ [Finetuning] Backbone frozen for warmup. Training only Readout layer.")

    def finetune_function(self, pl_module, current_epoch, optimizer):
        # åœ¨æŒ‡å®š epoch è§£å†»
        if current_epoch == self.unfreeze_at_epoch:
            self.unfreeze_and_add_param_group(
                modules=pl_module.model,
                optimizer=optimizer,
                train_bn=True, # è§£å†»åå…è®¸ BN æ›´æ–°
            )
            MLOGI(f"ğŸ”¥ [Finetuning] Backbone unfrozen at epoch {current_epoch}. Full finetuning started.")

# ==============================================================================
# è¾…åŠ©å‡½æ•°
# ==============================================================================
def find_best_ckpt(save_dir: str) -> str:
    """æŸ¥æ‰¾æœ€ä¼˜æˆ–æœ€æ–°çš„ Checkpoint æ–‡ä»¶ï¼Œä¼˜å…ˆ best.ckpt"""
    best = os.path.join(save_dir, 'best.ckpt')
    if os.path.exists(best): return best
    
    last = os.path.join(save_dir, 'last.ckpt')
    if os.path.exists(last): return last
    
    cpts = sorted(glob.glob(os.path.join(save_dir, '*.ckpt')))
    if len(cpts) > 0: return cpts[-1]
        
    raise FileNotFoundError(f'No checkpoint found in {save_dir}')

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Train SCWDS Probabilistic SimVP Model (Transfer Learning)')
    
    # --- åŸºç¡€è·¯å¾„ä¸æ•°æ®å‚æ•° ---
    parser.add_argument('--data_path', type=str, default='data/samples.jsonl', help='Path to training data')
    parser.add_argument('--save_dir', type=str, default='./output/prob_simvp', help='Output directory')
    parser.add_argument('--in_shape', type=int, nargs=4, default=None) 
    parser.add_argument('--batch_size', type=int, default=None)
    parser.add_argument('--max_epochs', type=int, default=30, help='Total training epochs')
    parser.add_argument('--num_workers', type=int, default=None)
    parser.add_argument('--aft_seq_length', type=int, default=None)

    # --- [æ ¸å¿ƒæ”¹è¿›] æ¦‚ç‡åˆ†ç®±ä¸ Loss å‚æ•° ---
    parser.add_argument('--num_bins', type=int, default=40, help='æ¦‚ç‡åˆ†ç®±æ•° (å»ºè®® 40)')
    parser.add_argument('--sigma', type=float, default=2.0, help='Soft Label é«˜æ–¯æ ‡å‡†å·® (å»ºè®® 2.0)')
    parser.add_argument('--use_focal', type=str, default='true', help='å¯ç”¨ Focal Loss (true/false)')
    parser.add_argument('--gamma', type=float, default=2.0, help='Focal Loss èšç„¦å‚æ•°')

    # --- [è¿ç§»å­¦ä¹ å‚æ•°] ---
    parser.add_argument('--base_ckpt_dir', type=str, required=True, help='SimVPåŸºåº§æ¨¡å‹ç›®å½• (ç”¨äºåŠ è½½æƒé‡)')
    parser.add_argument('--ckpt_path', type=str, default=None, help='æŒ‡å®šåŠ è½½çš„ Checkpoint è·¯å¾„ (Resume)')
    parser.add_argument('--unfreeze_epoch', type=int, default=3, help='è§£å†» Backbone çš„ Epoch (Warmup è½®æ•°)')
    
    # --- æ¨¡å‹ç»“æ„å‚æ•° ---
    parser.add_argument('--model_type', type=str, default=None)
    parser.add_argument('--hid_S', type=int, default=None)
    parser.add_argument('--hid_T', type=int, default=None)
    parser.add_argument('--N_S', type=int, default=None)
    parser.add_argument('--N_T', type=int, default=None)
    parser.add_argument('--mlp_ratio', type=float, default=None)
    parser.add_argument('--drop', type=float, default=None)
    parser.add_argument('--drop_path', type=float, default=None)
    
    # --- ä¼˜åŒ–å™¨ ---
    parser.add_argument('--opt', type=str, default='adamw')
    parser.add_argument('--lr', type=float, default=2e-4, help='åˆå§‹å­¦ä¹ ç‡ (å»ºè®® 2e-4)')
    parser.add_argument('--sched', type=str, default='cosine')
    parser.add_argument('--min_lr', type=float, default=1e-5)
    parser.add_argument('--warmup_epoch', type=int, default=0, help='LR Warmup (æ³¨æ„ä¸ Backbone Warmup åŒºåˆ†)')
    parser.add_argument('--accumulate_grad_batches', type=int, default=1)
    parser.add_argument('--gradient_clip_val', type=float, default=1.0)
    
    # --- è®¾å¤‡ä¸ç²¾åº¦ ---
    parser.add_argument('--accelerator', type=str, default='cuda')
    parser.add_argument('--devices', type=str, default='auto')
    parser.add_argument('--precision', type=str, default='bf16-mixed')
    
    # --- æ—©åœ ---
    parser.add_argument('--early_stop_patience', type=int, default=10)
    parser.add_argument('--early_stop_monitor', type=str, default='val_score')
    parser.add_argument('--early_stop_mode', type=str, default='max')

    return parser.parse_args()

# ==============================================================================
# ä¸»ç¨‹åº
# ==============================================================================
def main():
    torch.set_float32_matmul_precision('high')
    args = parse_args()
    
    # 1. å‚æ•°é¢„å¤„ç†
    config_kwargs = {k: v for k, v in vars(args).items() if v is not None}
    
    if 'in_shape' in config_kwargs: config_kwargs['in_shape'] = tuple(config_kwargs['in_shape'])
    
    # å¸ƒå°”å€¼è§£æ
    if isinstance(config_kwargs.get('use_focal'), str):
        config_kwargs['use_focal'] = config_kwargs['use_focal'].lower() == 'true'

    # å¼ºåˆ¶åŒæ­¥ num_bins åˆ° out_channels
    num_bins = config_kwargs.get('num_bins', 40)
    config_kwargs['out_channels'] = num_bins 
    
    # 2. åˆå§‹åŒ– Config
    try:
        # ç§»é™¤ Config ç±»ä¸æ¥å—çš„é¢å¤–å‚æ•° (å¦‚ sigma, use_focal, unfreeze_epoch ç­‰)
        valid_keys = SimVPConfig.model_fields.keys()
        safe_kwargs = {k: v for k, v in config_kwargs.items() if k in valid_keys}
        
        config = SimVPConfig(**safe_kwargs)
    except ValidationError as e:
        MLOGI(f"[ERROR] Config Validation: {e}")
        return

    l.seed_everything(config.seed)

    # 3. åˆå§‹åŒ– DataModule
    data_module = ScwdsDataModule(
        data_path=config.data_path,
        resize_shape=config.resize_shape,
        batch_size=config.batch_size,
        num_workers=config.num_workers,
        train_split=config.train_split,
        val_split=config.val_split,
        test_split=config.test_split,
        seed=config.seed
    )
    
    # 4. åˆå§‹åŒ–æ¨¡å‹ (æ‰‹åŠ¨æ³¨å…¥ Loss å‚æ•°)
    model_args = config.to_dict()
    
    # [å…³é”®] æ³¨å…¥ ProbabilisticSimVP æ‰€éœ€çš„ç‰¹å®šå‚æ•°
    model_args['num_bins'] = num_bins
    model_args['sigma'] = config_kwargs.get('sigma', 2.0)
    model_args['use_focal'] = config_kwargs.get('use_focal', True)
    model_args['gamma'] = config_kwargs.get('gamma', 2.0)
    
    MLOGI(f"[Init] Model: Bins={num_bins}, Sigma={model_args['sigma']}, Focal={model_args['use_focal']}")
    
    model = ProbabilisticSimVP(**model_args)

    # 5. è¿ç§»å­¦ä¹ ï¼šåŠ è½½åŸºåº§æƒé‡ (Backbone Loading)
    base_ckpt_path = args.ckpt_path # å¦‚æœæŒ‡å®šäº†ç‰¹å®š ckptï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨
    
    if base_ckpt_path is None:
        try:
            base_ckpt_path = find_best_ckpt(args.base_ckpt_dir)
        except FileNotFoundError:
            MLOGI(f"[WARNING] æœªåœ¨ {args.base_ckpt_dir} æ‰¾åˆ° Checkpointï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
            base_ckpt_path = None
    
    if base_ckpt_path:
        MLOGI(f"ğŸš€ å¯ç”¨è¿ç§»å­¦ä¹ : è½½å…¥ SimVP åŸºåº§æƒé‡è‡ª: {base_ckpt_path}")
        try:
            ckpt = torch.load(base_ckpt_path, map_location='cpu')
            state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt
            
            new_state_dict = {}
            for k, v in state_dict.items():
                # è¿‡æ»¤æ‰ Readout å±‚ (å› ä¸ºè¾“å‡ºé€šé“æ•°ä¸åŒ¹é…: 1 vs 40)
                if 'model.readout' in k: 
                    continue
                new_state_dict[k] = v

            # è½½å…¥ Backbone æƒé‡ (strict=False å…è®¸ç¼ºå¤± readout)
            model.load_state_dict(new_state_dict, strict=False)
            MLOGI("[INFO] Backbone æƒé‡åŠ è½½æˆåŠŸã€‚Readout å±‚å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹å­¦ä¹ ã€‚")
            
        except Exception as e:
            MLOGI(f"[ERROR] åŠ è½½åŸºåº§æ¨¡å‹æƒé‡å¤±è´¥: {e}ã€‚å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒã€‚")

    # 6. é…ç½® Callbacks
    monitor_metric = config.early_stop_monitor
    monitor_mode = config.early_stop_mode

    callbacks = [
        # [å…³é”®] è¿ç§»å­¦ä¹ å¾®è°ƒç­–ç•¥
        ProbabilisticFinetuning(unfreeze_at_epoch=args.unfreeze_epoch),
        
        # æ—©åœç­–ç•¥
        EarlyStopping(
            monitor=monitor_metric, 
            min_delta=config.early_stop_min_delta, 
            patience=config.early_stop_patience, 
            mode=monitor_mode, 
            verbose=True
        ),
        
        # æƒé‡ä¿å­˜
        ModelCheckpoint(
            dirpath=config.save_dir, 
            filename="prob-{epoch:02d}-{val_score:.4f}",
            monitor=monitor_metric,
            save_top_k=3, 
            mode=monitor_mode,
            save_last=True 
        ),
        
        LearningRateMonitor(logging_interval="step")
    ]

    logger = TensorBoardLogger(save_dir=config.save_dir, name=config.model_name, version=datetime.now().strftime("%Y%m%d-%H%M%S"))

    # DDP ç­–ç•¥é…ç½®
    strategy = 'ddp_find_unused_parameters_false' if config.devices != 1 and config.accelerator == 'cuda' else 'auto'

    # 7. åˆå§‹åŒ– Trainer
    trainer = l.Trainer(
        max_epochs=config.max_epochs,
        default_root_dir=config.save_dir,
        precision=config.precision,
        accelerator=config.accelerator,
        devices=config.devices,
        callbacks=callbacks,
        logger=logger,
        log_every_n_steps=config.log_every_n_steps,
        val_check_interval=config.val_check_interval,
        gradient_clip_val=config.gradient_clip_val,
        strategy=strategy,
        sync_batchnorm=False, 
        enable_progress_bar=config.enable_progress_bar,
        enable_model_summary=config.enable_model_summary,
        num_sanity_val_steps=config.num_sanity_val_steps,
    )

    MLOGI(f"Starting Training with Unfreeze Epoch: {args.unfreeze_epoch}")
    
    # 8. å¯åŠ¨è®­ç»ƒ
    # æ³¨æ„ï¼šå¦‚æœ args.ckpt_path è¢«æŒ‡å®šä¸”æ˜¯ä¸ºäº† Resume (è€Œéè¿ç§»å­¦ä¹ ), è¿™é‡Œåº”è¯¥ä¼ ç»™ ckpt_path å‚æ•°
    # ä½†æ ¹æ®å½“å‰é€»è¾‘ï¼Œargs.ckpt_path ç”¨äºè¿ç§»å­¦ä¹ åŠ è½½ï¼Œæ‰€ä»¥ Trainer.fit ä¸ä¼  ckpt_path (ä»å¤´å¼€å§‹ epoch è®¡æ•°)
    trainer.fit(model, datamodule=data_module)

if __name__ == "__main__":
    main()